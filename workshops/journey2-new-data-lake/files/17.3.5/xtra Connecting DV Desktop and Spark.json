{"paragraphs":[{"text":"%md\n# xtra tutorial: Working with Oracle Data Visualization Desktop and Spark\n\nThis tutorial was built for BDCS-CE version 17.3.5-20 as part of the New Data Lake User Journey: <a href=\"https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake\" target=\"_blank\">here</a>.  Questions and feedback about the tutorial: <david.bayard@oracle.com>\n\n\n    Note: This tutorial assumes you have tables defined in the Hive metastore which you should have if you ran the earlier tutorials.\n\nOracle Data Visualization Desktop ( <a href=\"https://docs.oracle.com/middleware/bidv1221/desktop/index.html\" target=\"_blank\">here</a> ) is a lightweight, single-file download tool to easily analyze data.  Data Visualization Desktop can connect to a variety of data sources.  In this tutorial, we will show you how you can use it to securely connect to BDCS-CE.  We will connect via DV Desktop's support for Spark.  \n\nPlease follow the instructions in the xtra Connecting DV Desktop and Hive for the DVD download and install instructions.\n","dateUpdated":"2017-09-28T18:07:35+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>xtra tutorial: Working with Oracle Data Visualization Desktop and Spark</h1>\n<p>This tutorial was built for BDCS-CE version 17.3.5-20 as part of the New Data Lake User Journey: <a href=\"https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake\" target=\"_blank\">here</a>. Questions and feedback about the tutorial: <a href=\"mailto:&#x64;&#97;v&#x69;&#x64;&#x2e;&#98;ay&#x61;r&#100;&#64;or&#97;&#x63;&#x6c;&#x65;&#x2e;&#99;&#111;&#x6d;\">&#x64;&#97;v&#x69;&#x64;&#x2e;&#98;ay&#x61;r&#100;&#64;or&#97;&#x63;&#x6c;&#x65;&#x2e;&#99;&#111;&#x6d;</a></p>\n<pre><code>Note: This tutorial assumes you have tables defined in the Hive metastore which you should have if you ran the earlier tutorials.\n</code></pre>\n<p>Oracle Data Visualization Desktop ( <a href=\"https://docs.oracle.com/middleware/bidv1221/desktop/index.html\" target=\"_blank\">here</a> ) is a lightweight, single-file download tool to easily analyze data. Data Visualization Desktop can connect to a variety of data sources. In this tutorial, we will show you how you can use it to securely connect to BDCS-CE. We will connect via DV Desktop&rsquo;s support for Spark. </p>\n<p>Please follow the instructions in the xtra Connecting DV Desktop and Hive for the DVD download and install instructions.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1506622055195_-521308988","id":"20170504-171842_974565851","dateCreated":"2017-09-28T18:07:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:348"},{"text":"%md\n# Configuring the Spark Thrift Server process to use binary transport\n\nIn order to connect to Spark from DVD, we need to configure BDCS-CE's Spark Thrift Server to use the binary transport protocol.  By default, BDCS-CE's spark thrift server is configured to use the http transport protocol.  These changes are done using the Ambari web console.\n\nHere are the steps:\n\n1.Follow the note \"xtra Connecting to Ambari\" to login to Ambari.\n2.Once connected to Ambari, click on \"Spark2\" on the left-hand list of services\n3.Then click on the \"Configs\" tab\n4.In the search box, type \"server2\"\n5.In the Advanced spark2-hive-site-override section, change the \"hive.server2.transport.mode\" to binary.\n6.In the Custom spark2-hive-site-override section, remove the property \"hive.server2.thrift.bind.host\" (by clicking on the red - symbol)\n7.Clear out the search box.  Then navigate down to the Custom spark2-thrift-sparkconf section\n8.In the Custom spark2-thrift-sparkconf section, click on the \"Add Property...\" link and then add the property \"spark.sql.shuffle.partitions=4\" and click Add. \n**Hint: You might also want to spark_daemon_memory to 2048 MB under the Advanced spark2-env section.**\n9.Click Save at the top of the screen.\n10.In the notes field, enter \"switch to binary transport\"\n11.Click save again\n12.If you see a \"Configurations\" pop-up, click \"Proceed Anyway\"\n13.Click OK to acknowledge that changes were made successfully\n14.Then click Restart, then Restart All Affected\n15.Then click Confirm Restart All\n\n![AmbariSparkBinary](https://raw.githubusercontent.com/millerhoo/journey2-new-data-lake/master/workshops/journey2-new-data-lake/images/300/AmbariSparkBinary.gif)\n\n\n","user":"anonymous","dateUpdated":"2017-09-29T18:10:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Configuring the Spark Thrift Server process to use binary transport</h1>\n<p>In order to connect to Spark from DVD, we need to configure BDCS-CE&rsquo;s Spark Thrift Server to use the binary transport protocol. By default, BDCS-CE&rsquo;s spark thrift server is configured to use the http transport protocol. These changes are done using the Ambari web console.</p>\n<p>Here are the steps:</p>\n<p>1.Follow the note &ldquo;xtra Connecting to Ambari&rdquo; to login to Ambari.<br/>2.Once connected to Ambari, click on &ldquo;Spark2&rdquo; on the left-hand list of services<br/>3.Then click on the &ldquo;Configs&rdquo; tab<br/>4.In the search box, type &ldquo;server2&rdquo;<br/>5.In the Advanced spark2-hive-site-override section, change the &ldquo;hive.server2.transport.mode&rdquo; to binary.<br/>6.In the Custom spark2-hive-site-override section, remove the property &ldquo;hive.server2.thrift.bind.host&rdquo; (by clicking on the red - symbol)<br/>7.Clear out the search box. Then navigate down to the Custom spark2-thrift-sparkconf section<br/>8.In the Custom spark2-thrift-sparkconf section, click on the &ldquo;Add Property&hellip;&rdquo; link and then add the property &ldquo;spark.sql.shuffle.partitions=4&rdquo; and click Add.<br/><strong>Hint: You might also want to spark_daemon_memory to 2048 MB under the Advanced spark2-env section.</strong><br/>9.Click Save at the top of the screen.<br/>10.In the notes field, enter &ldquo;switch to binary transport&rdquo;<br/>11.Click save again<br/>12.If you see a &ldquo;Configurations&rdquo; pop-up, click &ldquo;Proceed Anyway&rdquo;<br/>13.Click OK to acknowledge that changes were made successfully<br/>14.Then click Restart, then Restart All Affected<br/>15.Then click Confirm Restart All</p>\n<p><img src=\"https://raw.githubusercontent.com/millerhoo/journey2-new-data-lake/master/workshops/journey2-new-data-lake/images/300/AmbariSparkBinary.gif\" alt=\"AmbariSparkBinary\" /></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1506622055196_-523232733","id":"20170911-203514_1066756941","dateCreated":"2017-09-28T18:07:35+0000","dateStarted":"2017-09-29T18:10:33+0000","dateFinished":"2017-09-29T18:10:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:349"},{"text":"%md\n# Connecting to the Spark Thrift Server port (10016)\n\nNow, you need to decide how you want to connect to the Spark Thrift Server port, which is port 10016.  You can either choose to use a SSH tunnel (which is very secure) or choose to open port 10016 to the outside world (which can be less secure).\n\n+ If you want to use a SSH tunnel, refer to the note \"xtra Connecting to Ambari\" which has an example of setting up a SSH tunnel (but you would use port 10016 instead of Ambari's 8080).\n![SSHSparkBinary](https://raw.githubusercontent.com/millerhoo/journey2-new-data-lake/master/workshops/journey2-new-data-lake/images/300/SSHSparkBinary.gif)\n+ If instead of a SSH tunnel, you want to open up port 10016 to the internet, then you will need to create a new access rule for port 10016.  Refer to the note \"xtra Connecting via SSH\" or \"OEHCS Tutorial 1\" for examples of working with network access rules.\n\n","user":"anonymous","dateUpdated":"2017-09-29T13:43:56+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Connecting to the Spark Thrift Server port (10016)</h1>\n<p>Now, you need to decide how you want to connect to the Spark Thrift Server port, which is port 10016. You can either choose to use a SSH tunnel (which is very secure) or choose to open port 10016 to the outside world (which can be less secure).</p>\n<ul>\n  <li>If you want to use a SSH tunnel, refer to the note &ldquo;xtra Connecting to Ambari&rdquo; which has an example of setting up a SSH tunnel (but you would use port 10016 instead of Ambari&rsquo;s 8080).<br/><img src=\"https://raw.githubusercontent.com/millerhoo/journey2-new-data-lake/master/workshops/journey2-new-data-lake/images/300/SSHSparkBinary.gif\" alt=\"SSHSparkBinary\" /></li>\n  <li>If instead of a SSH tunnel, you want to open up port 10016 to the internet, then you will need to create a new access rule for port 10016. Refer to the note &ldquo;xtra Connecting via SSH&rdquo; or &ldquo;OEHCS Tutorial 1&rdquo; for examples of working with network access rules.</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1506622055197_-523617482","id":"20170911-204125_1313643018","dateCreated":"2017-09-28T18:07:35+0000","dateStarted":"2017-09-29T13:43:56+0000","dateFinished":"2017-09-29T13:43:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:350"},{"text":"%md\n# Define a connection in DV Desktop for the Spark connection\n\n+ Open up DV Desktop\n+ Click on Data Sources\n+ Click on Connection (Under Create)\n+ Click on Spark\n+ Enter the Connection Name\n+ Enter the Host Name.  If you are using SSH tunneling, then enter 127.0.0.1 or localhost.  If you have opened up port 10016, then use the IP for your BDCSCE instance.\n+ Enter the Port. It should now be 10016.\n+ Enter spark for the username\n+ Enter x for the password\n\n![DVDconnection](https://raw.githubusercontent.com/millerhoo/journey2-new-data-lake/master/workshops/journey2-new-data-lake/images/300/snap0012378.jpg)\n\n\n\n\n","user":"anonymous","dateUpdated":"2017-09-29T13:46:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Define a connection in DV Desktop for the Spark connection</h1>\n<ul>\n  <li>Open up DV Desktop</li>\n  <li>Click on Data Sources</li>\n  <li>Click on Connection (Under Create)</li>\n  <li>Click on Spark</li>\n  <li>Enter the Connection Name</li>\n  <li>Enter the Host Name. If you are using SSH tunneling, then enter 127.0.0.1 or localhost. If you have opened up port 10016, then use the IP for your BDCSCE instance.</li>\n  <li>Enter the Port. It should now be 10016.</li>\n  <li>Enter spark for the username</li>\n  <li>Enter x for the password</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/millerhoo/journey2-new-data-lake/master/workshops/journey2-new-data-lake/images/300/snap0012378.jpg\" alt=\"DVDconnection\" /></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1506622055197_-523617482","id":"20170728-174228_1743240708","dateCreated":"2017-09-28T18:07:35+0000","dateStarted":"2017-09-29T13:46:04+0000","dateFinished":"2017-09-29T13:46:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:351"},{"text":"%md\n# Create a DV Desktop Data Source for your connection\n\n**NOTE: With BDCS 17.3.5 and DVD3, you might need to follow the workaround in the following paragraph**\n\n+ Invoke the pop-up menu on your new Data Source and choose Create Data Source\n+ Navigate through the database, tables, and columns to choose the elements you want to add.\n+ Once you have selected your table and columns, click on the rightmost icon in the dataflow pipeline (it will be the icon after the filter icon).  Then, click on the Refresh property.  Change this to be \"Live - Always use the database\".  \n+ Name the new data source and Add it\n\n![DVDdatasource](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/SparkDataSource.jpg \"HODBC\")\n","user":"anonymous","dateUpdated":"2017-09-29T13:54:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Create a DV Desktop Data Source for your connection</h1>\n<p><strong>NOTE: With BDCS 17.3.5 and DVD3, you might need to follow the workaround in the following paragraph</strong></p>\n<ul>\n  <li>Invoke the pop-up menu on your new Data Source and choose Create Data Source</li>\n  <li>Navigate through the database, tables, and columns to choose the elements you want to add.</li>\n  <li>Once you have selected your table and columns, click on the rightmost icon in the dataflow pipeline (it will be the icon after the filter icon). Then, click on the Refresh property. Change this to be &ldquo;Live - Always use the database&rdquo;.</li>\n  <li>Name the new data source and Add it</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/SparkDataSource.jpg\" alt=\"DVDdatasource\" title=\"HODBC\" /></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1506622055198_-522463235","id":"20170731-202416_1602004865","dateCreated":"2017-09-28T18:07:35+0000","dateStarted":"2017-09-29T13:54:19+0000","dateFinished":"2017-09-29T13:54:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:352"},{"text":"%md\n# Tip - DVD 3.0 with Spark 2.1 no databases appear\n\nThis may be https://issues.apache.org/jira/browse/SPARK-9686\n\nIn any case, we have noticed that no databases appear when querying Spark2.1 from DVD 3.0.\n\nThere is a workaround...\n\nWhen you create your data source, use the \"Enter SQL\" feature to define your sql.  It can be as simple as a \"select * from tablename\" or more complex.  \n\nAnd once you have entered your sql, be sure to click on the rightmost icon in the dataflow pipeline (it will be the icon after the filter icon). Then, click on the Refresh property. Change this to be “Live - Always use the database”. \n\nHere is a video:\n![SparkCatalogWorkaround](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/SparkCatalogWorkaround.gif)\n\n\n\n","user":"anonymous","dateUpdated":"2017-09-29T13:53:07+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Tip - DVD 3.0 with Spark 2.1 no databases appear</h1>\n<p>This may be <a href=\"https://issues.apache.org/jira/browse/SPARK-9686\">https://issues.apache.org/jira/browse/SPARK-9686</a></p>\n<p>In any case, we have noticed that no databases appear when querying Spark2.1 from DVD 3.0.</p>\n<p>There is a workaround&hellip;</p>\n<p>When you create your data source, use the &ldquo;Enter SQL&rdquo; feature to define your sql. It can be as simple as a &ldquo;select * from tablename&rdquo; or more complex. </p>\n<p>And once you have entered your sql, be sure to click on the rightmost icon in the dataflow pipeline (it will be the icon after the filter icon). Then, click on the Refresh property. Change this to be “Live - Always use the database”. </p>\n<p>Here is a video:<br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/SparkCatalogWorkaround.gif\" alt=\"SparkCatalogWorkaround\" /></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1506622055198_-522463235","id":"20170911-210523_432676084","dateCreated":"2017-09-28T18:07:35+0000","dateStarted":"2017-09-29T13:53:07+0000","dateFinished":"2017-09-29T13:53:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:353"},{"text":"%md\n# Tip - Tracking Spark queries\n\nRun the following shell paragraph to peak at queries sent to the Spark thrift server.\n\n\n","dateUpdated":"2017-09-28T18:07:35+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Tip - Tracking Spark queries</h1>\n<p>Run the following shell paragraph to peak at queries sent to the Spark thrift server.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1506622055199_-522847984","id":"20170911-204536_1923836389","dateCreated":"2017-09-28T18:07:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:354"},{"title":"Shell command to peak at queries sent to Spark Thrift Server","text":"%sh\negrep $'Running|\\x0d|limit' /data/var/log/spark2-thrift/spark-hive-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-*-1.out | tail -400\n","user":"anonymous","dateUpdated":"2017-09-29T16:46:47+0000","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":"false"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"          from \r\n               (select sum(D1000002.c1) as `c1`,\r\n                         avg(D1000002.c1) as `c2`\r\n                    from \r\n                         (select T1000002.tripduration as `c1`\r\n                              from \r\n                                   (select * from bike_trips bike_trips) T1000002\r\n                         ) D1000002\r\n               ) D101\r\n     ) D103 limit 5000001' with e6107af2-7bd2-40c7-9934-5a18d6bd2ea2\n17/09/29 16:35:38 INFO SparkSqlParser: Parsing command: select 0 as `c1`,\r\n     D103.c1 as `c2`,\r\n     D103.c2 as `c3`,\r\n     D103.c3 as `c4`,\r\n     D103.c4 as `c5`\r\nfrom \r\n     (select distinct case  when D101.c1 < 0 then D101.c1 else 0 end  as `c1`,\r\n               case  when D101.c1 > 0 then D101.c1 else 0 end  as `c2`,\r\n               D101.c2 as `c3`,\r\n               D101.c1 as `c4`\r\n          from \r\n               (select sum(D1000002.c1) as `c1`,\r\n                         avg(D1000002.c1) as `c2`\r\n                    from \r\n                         (select T1000002.tripduration as `c1`\r\n                              from \r\n                                   (select * from bike_trips bike_trips) T1000002\r\n                         ) D1000002\r\n               ) D101\r\n     ) D103 limit 5000001\n17/09/29 16:35:50 INFO SparkExecuteStatementOperation: Running query 'use default' with 501cbef0-f46b-4938-a4ac-a922b93c36d5\n17/09/29 16:35:51 INFO SparkExecuteStatementOperation: Running query 'select 0 as `c1`,\r\n     D102.c1 as `c2`,\r\n     D102.c2 as `c3`,\r\n     D102.c3 as `c4`,\r\n     D102.c4 as `c5`,\r\n     D102.c5 as `c6`,\r\n     D102.c6 as `c7`\r\nfrom \r\n     (select distinct case  when D101.c1 < 0 then D101.c1 else 0 end  as `c1`,\r\n               case  when D101.c1 > 0 then D101.c1 else 0 end  as `c2`,\r\n               case  when D101.c2 < 0 then D101.c2 else 0 end  as `c3`,\r\n               case  when D101.c2 > 0 then D101.c2 else 0 end  as `c4`,\r\n               D101.c1 as `c5`,\r\n               D101.c2 as `c6`\r\n          from \r\n               (select avg(D1000002.c1) as `c1`,\r\n                         sum(D1000002.c1) as `c2`\r\n                    from \r\n                         (select T1000002.tripduration as `c1`\r\n                              from \r\n                                   (select * from bike_trips bike_trips) T1000002\r\n                         ) D1000002\r\n               ) D101\r\n     ) D102 limit 5000001' with 34d408ed-6a3a-41a6-a709-20ce8a5a42fa\n17/09/29 16:35:51 INFO SparkSqlParser: Parsing command: select 0 as `c1`,\r\n     D102.c1 as `c2`,\r\n     D102.c2 as `c3`,\r\n     D102.c3 as `c4`,\r\n     D102.c4 as `c5`,\r\n     D102.c5 as `c6`,\r\n     D102.c6 as `c7`\r\nfrom \r\n     (select distinct case  when D101.c1 < 0 then D101.c1 else 0 end  as `c1`,\r\n               case  when D101.c1 > 0 then D101.c1 else 0 end  as `c2`,\r\n               case  when D101.c2 < 0 then D101.c2 else 0 end  as `c3`,\r\n               case  when D101.c2 > 0 then D101.c2 else 0 end  as `c4`,\r\n               D101.c1 as `c5`,\r\n               D101.c2 as `c6`\r\n          from \r\n               (select avg(D1000002.c1) as `c1`,\r\n                         sum(D1000002.c1) as `c2`\r\n                    from \r\n                         (select T1000002.tripduration as `c1`\r\n                              from \r\n                                   (select * from bike_trips bike_trips) T1000002\r\n                         ) D1000002\r\n               ) D101\r\n     ) D102 limit 5000001\n17/09/29 16:37:36 INFO SparkExecuteStatementOperation: Running query 'use default' with 67b617df-d298-400e-a234-27c922a399fb\n17/09/29 16:37:36 INFO SparkExecuteStatementOperation: Running query 'select 0 as `c1`,\r\n     1 as `c2`,\r\n     ROW_NUMBER() OVER ( ORDER BY D101.c1) as `c3`,\r\n     D101.c1 as `c4`,\r\n     D101.c2 as `c5`,\r\n     D101.c3 as `c6`,\r\n     D101.c4 as `c7`,\r\n     D101.c5 as `c8`,\r\n     D101.c6 as `c9`,\r\n     D101.c7 as `c10`,\r\n     D101.c8 as `c11`,\r\n     D101.c9 as `c12`,\r\n     D101.c10 as `c13`,\r\n     D101.c11 as `c14`,\r\n     D101.c12 as `c15`,\r\n     D101.c13 as `c16`,\r\n     D101.c14 as `c17`,\r\n     D101.c15 as `c18`\r\nfrom \r\n     (select T1000001.bikeid as `c1`,\r\n               T1000001.birthyear as `c2`,\r\n               T1000001.endstationid as `c3`,\r\n               T1000001.endstationlatitude as `c4`,\r\n               T1000001.endstationlongitude as `c5`,\r\n               T1000001.endstationname as `c6`,\r\n               T1000001.gender as `c7`,\r\n               T1000001.startstationid as `c8`,\r\n               T1000001.startstationlatitude as `c9`,\r\n               T1000001.startstationlongitude as `c10`,\r\n               T1000001.startstationname as `c11`,\r\n               T1000001.starttime as `c12`,\r\n               T1000001.stoptime as `c13`,\r\n               T1000001.tripduration as `c14`,\r\n               T1000001.usertype as `c15`\r\n          from \r\n               (select * from bike_trips bike_trips) T1000001\r\n     ) D101 limit 101' with 5ce2c80e-7a9b-47bb-b3c3-b4234280ac82\n17/09/29 16:37:36 INFO SparkSqlParser: Parsing command: select 0 as `c1`,\r\n     1 as `c2`,\r\n     ROW_NUMBER() OVER ( ORDER BY D101.c1) as `c3`,\r\n     D101.c1 as `c4`,\r\n     D101.c2 as `c5`,\r\n     D101.c3 as `c6`,\r\n     D101.c4 as `c7`,\r\n     D101.c5 as `c8`,\r\n     D101.c6 as `c9`,\r\n     D101.c7 as `c10`,\r\n     D101.c8 as `c11`,\r\n     D101.c9 as `c12`,\r\n     D101.c10 as `c13`,\r\n     D101.c11 as `c14`,\r\n     D101.c12 as `c15`,\r\n     D101.c13 as `c16`,\r\n     D101.c14 as `c17`,\r\n     D101.c15 as `c18`\r\nfrom \r\n     (select T1000001.bikeid as `c1`,\r\n               T1000001.birthyear as `c2`,\r\n               T1000001.endstationid as `c3`,\r\n               T1000001.endstationlatitude as `c4`,\r\n               T1000001.endstationlongitude as `c5`,\r\n               T1000001.endstationname as `c6`,\r\n               T1000001.gender as `c7`,\r\n               T1000001.startstationid as `c8`,\r\n               T1000001.startstationlatitude as `c9`,\r\n               T1000001.startstationlongitude as `c10`,\r\n               T1000001.startstationname as `c11`,\r\n               T1000001.starttime as `c12`,\r\n               T1000001.stoptime as `c13`,\r\n               T1000001.tripduration as `c14`,\r\n               T1000001.usertype as `c15`\r\n          from \r\n               (select * from bike_trips bike_trips) T1000001\r\n     ) D101 limit 101\n17/09/29 16:38:34 INFO SparkExecuteStatementOperation: Running query 'use default' with cb7abc18-58f8-4427-a380-1876b5fd915d\n17/09/29 16:38:34 INFO SparkExecuteStatementOperation: Running query 'select 0 as `c1`,\r\n     D103.c1 as `c2`,\r\n     D103.c2 as `c3`,\r\n     D103.c3 as `c4`,\r\n     0 as `c5`,\r\n     D103.c4 as `c6`\r\nfrom \r\n     (select D101.c1 as `c1`,\r\n               case  when D101.c2 < 0 then D101.c2 else 0 end  as `c2`,\r\n               case  when D101.c2 > 0 then D101.c2 else 0 end  as `c3`,\r\n               D101.c2 as `c4`\r\n          from \r\n               (select D102.c1 as `c1`,\r\n                         sum(1) as `c2`\r\n                    from \r\n                         (select distinct T1000002.startstationname as `c1`\r\n                              from \r\n                                   (select * from bike_trips bike_trips) T1000002\r\n                         ) D102\r\n                    group by D102.c1\r\n               ) D101\r\n17/09/29 16:38:34 INFO SparkSqlParser: Parsing command: select 0 as `c1`,\r\n     D103.c1 as `c2`,\r\n     D103.c2 as `c3`,\r\n     D103.c3 as `c4`,\r\n     0 as `c5`,\r\n     D103.c4 as `c6`\r\nfrom \r\n     (select D101.c1 as `c1`,\r\n               case  when D101.c2 < 0 then D101.c2 else 0 end  as `c2`,\r\n               case  when D101.c2 > 0 then D101.c2 else 0 end  as `c3`,\r\n               D101.c2 as `c4`\r\n          from \r\n               (select D102.c1 as `c1`,\r\n                         sum(1) as `c2`\r\n                    from \r\n                         (select distinct T1000002.startstationname as `c1`\r\n                              from \r\n                                   (select * from bike_trips bike_trips) T1000002\r\n                         ) D102\r\n                    group by D102.c1\r\n               ) D101\r\n17/09/29 16:38:52 INFO SparkExecuteStatementOperation: Running query 'use default' with 046aaebd-dcc6-430a-9ac6-2cd54c19ec0d\n17/09/29 16:38:53 INFO SparkExecuteStatementOperation: Running query 'select 0 as `c1`,\r\n     1 as `c2`,\r\n     D103.c1 as `c3`\r\nfrom \r\n     (select distinct T1000002.startstationname as `c1`\r\n          from \r\n               (select * from bike_trips bike_trips) T1000002\r\n     ) D103 limit 5000001' with 1d7dca26-e580-4078-b14a-c2975f4b8559\n17/09/29 16:38:53 INFO SparkSqlParser: Parsing command: select 0 as `c1`,\r\n     1 as `c2`,\r\n     D103.c1 as `c3`\r\nfrom \r\n     (select distinct T1000002.startstationname as `c1`\r\n          from \r\n               (select * from bike_trips bike_trips) T1000002\r\n     ) D103 limit 5000001\n17/09/29 16:39:27 INFO SparkExecuteStatementOperation: Running query 'use default' with 1554f0a1-bbd0-4486-8df1-6f1b7fe509f4\n17/09/29 16:39:28 INFO SparkExecuteStatementOperation: Running query 'select 0 as `c1`,\r\n     D103.c1 as `c2`,\r\n     D103.c2 as `c3`,\r\n     D103.c3 as `c4`,\r\n     D103.c4 as `c5`,\r\n     0 as `c6`\r\nfrom \r\n     (select distinct D101.c1 as `c1`,\r\n               case  when count(D101.c2) over ()  < 0 then count(D101.c2) over ()  else 0 end  as `c2`,\r\n               case  when count(D101.c2) over ()  > 0 then count(D101.c2) over ()  else 0 end  as `c3`,\r\n               count(D101.c2) over ()  as `c4`\r\n          from \r\n               (select D1000001.c1 as `c1`,\r\n                         avg(D1000001.c2) as `c2`\r\n                    from \r\n                         (select T1000002.startstationname as `c1`,\r\n                                   T1000002.tripduration as `c2`\r\n                              from \r\n                                   (select * from bike_trips bike_trips) T1000002\r\n                         ) D1000001\r\n                    group by D1000001.c1\r\n               ) D101\r\n17/09/29 16:39:28 INFO SparkSqlParser: Parsing command: select 0 as `c1`,\r\n     D103.c1 as `c2`,\r\n     D103.c2 as `c3`,\r\n     D103.c3 as `c4`,\r\n     D103.c4 as `c5`,\r\n     0 as `c6`\r\nfrom \r\n     (select distinct D101.c1 as `c1`,\r\n               case  when count(D101.c2) over ()  < 0 then count(D101.c2) over ()  else 0 end  as `c2`,\r\n               case  when count(D101.c2) over ()  > 0 then count(D101.c2) over ()  else 0 end  as `c3`,\r\n               count(D101.c2) over ()  as `c4`\r\n          from \r\n               (select D1000001.c1 as `c1`,\r\n                         avg(D1000001.c2) as `c2`\r\n                    from \r\n                         (select T1000002.startstationname as `c1`,\r\n                                   T1000002.tripduration as `c2`\r\n                              from \r\n                                   (select * from bike_trips bike_trips) T1000002\r\n                         ) D1000001\r\n                    group by D1000001.c1\r\n               ) D101\r\n17/09/29 16:40:24 INFO SparkExecuteStatementOperation: Running query 'use default' with fa1ea23c-0b9f-438d-880c-01aa22de6fe9\n17/09/29 16:40:25 INFO SparkExecuteStatementOperation: Running query 'select 0 as `c1`,\r\n     1 as `c2`,\r\n     ROW_NUMBER() OVER ( ORDER BY D101.c1) as `c3`,\r\n     D101.c1 as `c4`,\r\n     D101.c2 as `c5`,\r\n     D101.c3 as `c6`,\r\n     D101.c4 as `c7`,\r\n     D101.c5 as `c8`,\r\n     D101.c6 as `c9`,\r\n     D101.c7 as `c10`,\r\n     D101.c8 as `c11`,\r\n     D101.c9 as `c12`,\r\n     D101.c10 as `c13`,\r\n     D101.c11 as `c14`,\r\n     D101.c12 as `c15`,\r\n     D101.c13 as `c16`,\r\n     D101.c14 as `c17`,\r\n     D101.c15 as `c18`\r\nfrom \r\n     (select T1000001.bikeid as `c1`,\r\n               T1000001.birthyear as `c2`,\r\n               T1000001.endstationid as `c3`,\r\n               T1000001.endstationlatitude as `c4`,\r\n               T1000001.endstationlongitude as `c5`,\r\n               T1000001.endstationname as `c6`,\r\n               T1000001.gender as `c7`,\r\n               T1000001.startstationid as `c8`,\r\n               T1000001.startstationlatitude as `c9`,\r\n               T1000001.startstationlongitude as `c10`,\r\n               T1000001.startstationname as `c11`,\r\n               T1000001.starttime as `c12`,\r\n               T1000001.stoptime as `c13`,\r\n               T1000001.tripduration as `c14`,\r\n               T1000001.usertype as `c15`\r\n          from \r\n               (select * from bike_trips bike_trips) T1000001\r\n     ) D101 limit 101' with 567e2948-0433-4afc-ac28-0d796ae1c5a5\n17/09/29 16:40:25 INFO SparkSqlParser: Parsing command: select 0 as `c1`,\r\n     1 as `c2`,\r\n     ROW_NUMBER() OVER ( ORDER BY D101.c1) as `c3`,\r\n     D101.c1 as `c4`,\r\n     D101.c2 as `c5`,\r\n     D101.c3 as `c6`,\r\n     D101.c4 as `c7`,\r\n     D101.c5 as `c8`,\r\n     D101.c6 as `c9`,\r\n     D101.c7 as `c10`,\r\n     D101.c8 as `c11`,\r\n     D101.c9 as `c12`,\r\n     D101.c10 as `c13`,\r\n     D101.c11 as `c14`,\r\n     D101.c12 as `c15`,\r\n     D101.c13 as `c16`,\r\n     D101.c14 as `c17`,\r\n     D101.c15 as `c18`\r\nfrom \r\n     (select T1000001.bikeid as `c1`,\r\n               T1000001.birthyear as `c2`,\r\n               T1000001.endstationid as `c3`,\r\n               T1000001.endstationlatitude as `c4`,\r\n               T1000001.endstationlongitude as `c5`,\r\n               T1000001.endstationname as `c6`,\r\n               T1000001.gender as `c7`,\r\n               T1000001.startstationid as `c8`,\r\n               T1000001.startstationlatitude as `c9`,\r\n               T1000001.startstationlongitude as `c10`,\r\n               T1000001.startstationname as `c11`,\r\n               T1000001.starttime as `c12`,\r\n               T1000001.stoptime as `c13`,\r\n               T1000001.tripduration as `c14`,\r\n               T1000001.usertype as `c15`\r\n          from \r\n               (select * from bike_trips bike_trips) T1000001\r\n     ) D101 limit 101\n17/09/29 16:41:41 INFO SparkExecuteStatementOperation: Running query 'use default' with 956068aa-6f74-4c54-88e3-74d03ddad802\n17/09/29 16:41:42 INFO SparkExecuteStatementOperation: Running query 'select 0 as `c1`,\r\n     D103.c1 as `c2`,\r\n     D103.c2 as `c3`,\r\n     D103.c3 as `c4`\r\nfrom \r\n     (select distinct case  when count(D101.c1) over ()  < 0 then count(D101.c1) over ()  else 0 end  as `c1`,\r\n               case  when count(D101.c1) over ()  > 0 then count(D101.c1) over ()  else 0 end  as `c2`,\r\n               count(D101.c1) over ()  as `c3`\r\n          from \r\n               (select T1000002.tripduration as `c1`\r\n                    from \r\n                         (select * from bike_trips bike_trips) T1000002\r\n               ) D101\r\n     ) D103 limit 5000001' with f7dbc3c3-d1fe-4cf2-a2cd-9b5e401df938\n17/09/29 16:41:42 INFO SparkSqlParser: Parsing command: select 0 as `c1`,\r\n     D103.c1 as `c2`,\r\n     D103.c2 as `c3`,\r\n     D103.c3 as `c4`\r\nfrom \r\n     (select distinct case  when count(D101.c1) over ()  < 0 then count(D101.c1) over ()  else 0 end  as `c1`,\r\n               case  when count(D101.c1) over ()  > 0 then count(D101.c1) over ()  else 0 end  as `c2`,\r\n               count(D101.c1) over ()  as `c3`\r\n          from \r\n               (select T1000002.tripduration as `c1`\r\n                    from \r\n                         (select * from bike_trips bike_trips) T1000002\r\n               ) D101\r\n     ) D103 limit 5000001\n17/09/29 16:42:12 INFO SparkExecuteStatementOperation: Running query 'use default' with 72131a33-9a51-45a0-abf5-45f2fb30dcdc\n17/09/29 16:42:13 INFO SparkExecuteStatementOperation: Running query 'select 0 as `c1`,\r\n     D103.c1 as `c2`,\r\n     D103.c2 as `c3`,\r\n     D103.c3 as `c4`,\r\n     D103.c4 as `c5`,\r\n     0 as `c6`\r\nfrom \r\n     (select D101.c1 as `c1`,\r\n               case  when count(D101.c2) < 0 then count(D101.c2) else 0 end  as `c2`,\r\n               case  when count(D101.c2) > 0 then count(D101.c2) else 0 end  as `c3`,\r\n               count(D101.c2) as `c4`\r\n          from \r\n               (select T1000002.startstationname as `c1`,\r\n                         T1000002.tripduration as `c2`\r\n                    from \r\n                         (select * from bike_trips bike_trips) T1000002\r\n               ) D101\r\n          group by D101.c1\r\n17/09/29 16:42:13 INFO SparkSqlParser: Parsing command: select 0 as `c1`,\r\n     D103.c1 as `c2`,\r\n     D103.c2 as `c3`,\r\n     D103.c3 as `c4`,\r\n     D103.c4 as `c5`,\r\n     0 as `c6`\r\nfrom \r\n     (select D101.c1 as `c1`,\r\n               case  when count(D101.c2) < 0 then count(D101.c2) else 0 end  as `c2`,\r\n               case  when count(D101.c2) > 0 then count(D101.c2) else 0 end  as `c3`,\r\n               count(D101.c2) as `c4`\r\n          from \r\n               (select T1000002.startstationname as `c1`,\r\n                         T1000002.tripduration as `c2`\r\n                    from \r\n                         (select * from bike_trips bike_trips) T1000002\r\n               ) D101\r\n          group by D101.c1\r\n"}]},"apps":[],"jobName":"paragraph_1506622055199_-522847984","id":"20170911-204553_822044476","dateCreated":"2017-09-28T18:07:35+0000","dateStarted":"2017-09-29T16:46:47+0000","dateFinished":"2017-09-29T16:46:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:355"},{"text":"","user":"anonymous","dateUpdated":"2017-09-29T14:54:50+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1506622055200_-537083693","id":"20170504-171816_753503470","dateCreated":"2017-09-28T18:07:35+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:356"},{"dateUpdated":"2017-09-28T18:07:35+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1506622055200_-537083693","id":"20170921-124733_1996309865","dateCreated":"2017-09-28T18:07:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:357"}],"name":"xtra Connecting DV Desktop and Spark","id":"2CTQFVM54","angularObjects":{"2CTRFEEBW:shared_process":[],"2CU8TXPWB:shared_process":[],"2CVJJ9KVH:shared_process":[],"2CV3HJX2Y:shared_process":[],"2CWP7SFVD:shared_process":[],"2CUBFR26Z:shared_process":[],"2CW13EPN9:shared_process":[],"2C4U48MY3_spark2:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}