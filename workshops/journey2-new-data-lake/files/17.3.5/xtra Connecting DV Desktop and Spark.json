{"paragraphs":[{"text":"%md\n# xtra tutorial: Working with Oracle Data Visualization Desktop and Spark\n\nThis tutorial was built for BDCS-CE version 17.3.5-20 as part of the New Data Lake User Journey: <a href=\"https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake\" target=\"_blank\">here</a>.  Questions and feedback about the tutorial: <david.bayard@oracle.com>\n\n\n    Note: This tutorial assumes you have tables defined in the Hive metastore which you should have if you ran the earlier tutorials.\n\nOracle Data Visualization Desktop ( <a href=\"https://docs.oracle.com/middleware/bidv1221/desktop/index.html\" target=\"_blank\">here</a> ) is a lightweight, single-file download tool to easily analyze data.  Data Visualization Desktop can connect to a variety of data sources.  In this tutorial, we will show you how you can use it to securely connect to BDCS-CE.  We will connect via DV Desktop's support for Spark.  \n\nPlease follow the instructions in the xtra Connecting DV Desktop and Hive for the DVD download and install instructions.\n","user":"anonymous","dateUpdated":"2017-09-11T20:32:25+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>xtra tutorial: Working with Oracle Data Visualization Desktop and Spark</h1>\n<p>This tutorial was built for BDCS-CE version 17.3.5-20 as part of the New Data Lake User Journey: <a href=\"https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake\" target=\"_blank\">here</a>. Questions and feedback about the tutorial: <a href=\"mailto:&#x64;&#97;v&#x69;&#x64;&#x2e;&#98;ay&#x61;r&#100;&#64;or&#97;&#x63;&#x6c;&#x65;&#x2e;&#99;&#111;&#x6d;\">&#x64;&#97;v&#x69;&#x64;&#x2e;&#98;ay&#x61;r&#100;&#64;or&#97;&#x63;&#x6c;&#x65;&#x2e;&#99;&#111;&#x6d;</a></p>\n<pre><code>Note: This tutorial assumes you have tables defined in the Hive metastore which you should have if you ran the earlier tutorials.\n</code></pre>\n<p>Oracle Data Visualization Desktop ( <a href=\"https://docs.oracle.com/middleware/bidv1221/desktop/index.html\" target=\"_blank\">here</a> ) is a lightweight, single-file download tool to easily analyze data. Data Visualization Desktop can connect to a variety of data sources. In this tutorial, we will show you how you can use it to securely connect to BDCS-CE. We will connect via DV Desktop&rsquo;s support for Spark. </p>\n<p>Please follow the instructions in the xtra Connecting DV Desktop and Hive for the DVD download and install instructions.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1504646601869_1886148541","id":"20170504-171842_974565851","dateCreated":"2017-09-05T21:23:21+0000","dateStarted":"2017-09-11T20:32:25+0000","dateFinished":"2017-09-11T20:32:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:348"},{"text":"%md\n# Configuring the Spark Thrift Server process to use binary transport\n\nIn order to connect to Spark from DVD, we need to configure BDCS-CE's Spark Thrift Server to use the binary transport protocol.  By default, BDCS-CE's spark thrift server is configured to use the http transport protocol.  These changes are done using the Ambari web console.\n\nHere are the steps:\n\n1.Follow the note \"xtra Connecting to Ambari\" to login to Ambari.\n2.Once connected to Ambari, click on \"Spark2\" on the left-hand list of services\n3.Then click on the \"Configs\" tab\n4.In the search box, type \"server2\"\n5.In the Advanced spark2-hive-site-override section, change the \"hive.server2.transport.mode\" to binary.\n6.In the Custom spark2-hive-site-override section, remove the property \"hive.server2.thrift.bind.host\" (by clicking on the red - symbol)\n7.In the Custom spark2-thrift-sparkconf section, click on the \"Add Property...\" link and then add the property \"spark.sql.shuffle.partitions=4\" and click Add. **NOTE: THIS STEP IS NOT YET IN THE ANIMATED GIF BELOW**\n8.Click Save at the top of the screen.\n9.In the notes field, enter \"transport mode\"\n10.Click save again\n11.If you see a \"Configurations\" pop-up, click \"Proceed Anyway\"\n12.Click OK to acknowledge that changes were made successfully\n13.Then click Restart, then Restart All Affected\n14.Then click Confirm Restart All\n\n![AmbariSparkBinary](https://raw.githubusercontent.com/millerhoo/journey2-new-data-lake/master/workshops/journey2-new-data-lake/images/300/AmbariSparkBinary.gif)\n\n\n","user":"anonymous","dateUpdated":"2017-09-21T14:00:06+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":"true"},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Configuring the Spark Thrift Server process to use binary transport</h1>\n<p>In order to connect to Spark from DVD, we need to configure BDCS-CE&rsquo;s Spark Thrift Server to use the binary transport protocol. By default, BDCS-CE&rsquo;s spark thrift server is configured to use the http transport protocol. These changes are done using the Ambari web console.</p>\n<p>Here are the steps:</p>\n<p>1.Follow the note &ldquo;xtra Connecting to Ambari&rdquo; to login to Ambari.<br/>2.Once connected to Ambari, click on &ldquo;Spark2&rdquo; on the left-hand list of services<br/>3.Then click on the &ldquo;Configs&rdquo; tab<br/>4.In the search box, type &ldquo;server2&rdquo;<br/>5.In the Advanced spark2-hive-site-override section, change the &ldquo;hive.server2.transport.mode&rdquo; to binary.<br/>6.In the Custom spark2-hive-site-override section, remove the property &ldquo;hive.server2.thrift.bind.host&rdquo; (by clicking on the red - symbol)<br/>7.In the Custom spark2-thrift-sparkconf section, click on the &ldquo;Add Property&hellip;&rdquo; link and then add the property &ldquo;spark.sql.shuffle.partitions=4&rdquo; and click Add. <strong>NOTE: THIS STEP IS NOT YET IN THE ANIMATED GIF BELOW</strong><br/>8.Click Save at the top of the screen.<br/>9.In the notes field, enter &ldquo;transport mode&rdquo;<br/>10.Click save again<br/>11.If you see a &ldquo;Configurations&rdquo; pop-up, click &ldquo;Proceed Anyway&rdquo;<br/>12.Click OK to acknowledge that changes were made successfully<br/>13.Then click Restart, then Restart All Affected<br/>14.Then click Confirm Restart All</p>\n<p><img src=\"https://raw.githubusercontent.com/millerhoo/journey2-new-data-lake/master/workshops/journey2-new-data-lake/images/300/AmbariSparkBinary.gif\" alt=\"AmbariSparkBinary\" /></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1505162114449_-1401130739","id":"20170911-203514_1066756941","dateCreated":"2017-09-11T20:35:14+0000","dateStarted":"2017-09-21T14:00:06+0000","dateFinished":"2017-09-21T14:00:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:349"},{"text":"%md\n# Connecting to the Spark Thrift Server port (10016)\n\nNow, you need to decide how you want to connect to the Spark Thrift Server port, which is port 10016.  You can either choose to use a SSH tunnel (which is very secure) or choose to open port 10016 to the outside world (which can be less secure).\n\n+ If you want to use a SSH tunnel, refer to the note \"xtra Connecting to Ambari\" which has an example of setting up a SSH tunnel (but you would use port 10016 instead of Ambari's 8080).\n+ If you want to open up port 10016, you will need to create a new access rule for port 10016.  Refer to the note \"xtra Connecting via SSH\" or \"OEHCS Tutorial 1\" for examples of working with network access rules.\n\n","user":"anonymous","dateUpdated":"2017-09-11T20:43:05+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":"true"},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Connecting to the Spark Thrift Server port (10016)</h1>\n<p>Now, you need to decide how you want to connect to the Spark Thrift Server port, which is port 10016. You can either choose to use a SSH tunnel (which is very secure) or choose to open port 10016 to the outside world (which can be less secure).</p>\n<ul>\n  <li>If you want to use a SSH tunnel, refer to the note &ldquo;xtra Connecting to Ambari&rdquo; which has an example of setting up a SSH tunnel (but you would use port 10016 instead of Ambari&rsquo;s 8080).</li>\n  <li>If you want to open up port 10016, you will need to create a new access rule for port 10016. Refer to the note &ldquo;xtra Connecting via SSH&rdquo; or &ldquo;OEHCS Tutorial 1&rdquo; for examples of working with network access rules.</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1505162485921_1672546670","id":"20170911-204125_1313643018","dateCreated":"2017-09-11T20:41:25+0000","dateStarted":"2017-09-11T20:43:05+0000","dateFinished":"2017-09-11T20:43:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:350"},{"text":"%md\n# Define a connection in DV Desktop for the Spark connection\n\n+ Open up DV Desktop\n+ Click on Data Sources\n+ Click on Connection (Under Create)\n+ Click on Spark\n+ Enter the Connection Name\n+ Enter the Host Name.  If you are using SSH tunneling, then enter 127.0.0.1 or localhost.  If you have opened up port 10016, then use the IP for your BDCSCE instance.\n+ Enter the Port. It should now be 10016.\n+ Enter spark for the username\n+ Enter x for the password\n\n![DVDconnection](https://raw.githubusercontent.com/millerhoo/journey2-new-data-lake/master/workshops/journey2-new-data-lake/images/300/snap0012378.jpg)\n\n\n\n\n","user":"anonymous","dateUpdated":"2017-09-14T17:28:57+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Define a connection in DV Desktop for the Spark connection</h1>\n<ul>\n  <li>Open up DV Desktop</li>\n  <li>Click on Data Sources</li>\n  <li>Click on Connection (Under Create)</li>\n  <li>Click on Spark</li>\n  <li>Enter the Connection Name</li>\n  <li>Enter the Host Name. If you are using SSH tunneling, then enter 127.0.0.1 or localhost. If you have opened up port 10016, then use the IP for your BDCSCE instance.</li>\n  <li>Enter the Port. It should now be 10016.</li>\n  <li>Enter spark for the username</li>\n  <li>Enter x for the password</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/millerhoo/journey2-new-data-lake/master/workshops/journey2-new-data-lake/images/300/snap0012378.jpg\" alt=\"DVDconnection\" /></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1504646601870_1887302787","id":"20170728-174228_1743240708","dateCreated":"2017-09-05T21:23:21+0000","dateStarted":"2017-09-14T17:28:57+0000","dateFinished":"2017-09-14T17:28:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:351"},{"text":"%md\n# Create a DV Desktop Data Source for your connection\n\n+ Invoke the pop-up menu on your new Data Source and choose Create Data Source\n+ Navigate through the database, tables, and columns to choose the elements you want to add.\n+ Once you have selected your table and columns, click on the rightmost icon in the dataflow pipeline (it will be the icon after the filter icon).  Then, click on the Refresh property.  Change this to be \"Live - Always use the database\".  **NOTE: This step is not yet captured in the screenshot below**\n+ Name the new data source and Add it\n\n![DVDdatasource](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/SparkDataSource.jpg \"HODBC\")\n","user":"anonymous","dateUpdated":"2017-09-21T14:04:07+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Create a DV Desktop Data Source for your connection</h1>\n<ul>\n  <li>Invoke the pop-up menu on your new Data Source and choose Create Data Source</li>\n  <li>Navigate through the database, tables, and columns to choose the elements you want to add.</li>\n  <li>Once you have selected your table and columns, click on the rightmost icon in the dataflow pipeline (it will be the icon after the filter icon). Then, click on the Refresh property. Change this to be &ldquo;Live - Always use the database&rdquo;. <strong>NOTE: This step is not yet captured in the screenshot below</strong></li>\n  <li>Name the new data source and Add it</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/SparkDataSource.jpg\" alt=\"DVDdatasource\" title=\"HODBC\" /></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1504646601870_1887302787","id":"20170731-202416_1602004865","dateCreated":"2017-09-05T21:23:21+0000","dateStarted":"2017-09-21T14:04:07+0000","dateFinished":"2017-09-21T14:04:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:352"},{"text":"%md\n# Tip - DVD 3.0 with Spark 2.1 no databases appear\n\nThis may be https://issues.apache.org/jira/browse/SPARK-9686\n\nIn any case, we have noticed that no databases appear when querying Spark2.1 from DVD 3.0.\n\nThere is a workaround...\n\nWhen you create your data source, use the \"Enter SQL\" feature to define your sql.  It can be as simple as a \"select * from tablename\" or more complex.  \n\nAnd once you have entered your sql, be sure to click on the rightmost icon in the dataflow pipeline (it will be the icon after the filter icon). Then, click on the Refresh property. Change this to be “Live - Always use the database”. **NOTE: This step is not yet captured in the screenshot below**\n\nHere is a video:\n![SparkCatalogWorkaround](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/SparkCatalogWorkaround.gif)\n\n\n\n","user":"anonymous","dateUpdated":"2017-09-21T14:05:17+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":"true"},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Tip - DVD 3.0 with Spark 2.1 no databases appear</h1>\n<p>This may be <a href=\"https://issues.apache.org/jira/browse/SPARK-9686\">https://issues.apache.org/jira/browse/SPARK-9686</a></p>\n<p>In any case, we have noticed that no databases appear when querying Spark2.1 from DVD 3.0.</p>\n<p>There is a workaround&hellip;</p>\n<p>When you create your data source, use the &ldquo;Enter SQL&rdquo; feature to define your sql. It can be as simple as a &ldquo;select * from tablename&rdquo; or more complex. </p>\n<p>And once you have entered your sql, be sure to click on the rightmost icon in the dataflow pipeline (it will be the icon after the filter icon). Then, click on the Refresh property. Change this to be “Live - Always use the database”. <strong>NOTE: This step is not yet captured in the screenshot below</strong></p>\n<p>Here is a video:<br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/SparkCatalogWorkaround.gif\" alt=\"SparkCatalogWorkaround\" /></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1505163923030_2068908080","id":"20170911-210523_432676084","dateCreated":"2017-09-11T21:05:23+0000","dateStarted":"2017-09-21T14:05:17+0000","dateFinished":"2017-09-21T14:05:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:353"},{"text":"%md\n# Tip - Tracking Spark queries\n\nRun the following shell paragraph to peak at queries sent to the Spark thrift server.\n\n\n","user":"anonymous","dateUpdated":"2017-09-11T21:05:18+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":"true"},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Tip - Tracking Spark queries</h1>\n<p>Run the following shell paragraph to peak at queries sent to the Spark thrift server.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1505162736617_-1816222251","id":"20170911-204536_1923836389","dateCreated":"2017-09-11T20:45:36+0000","dateStarted":"2017-09-11T21:05:18+0000","dateFinished":"2017-09-11T21:05:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:354"},{"title":"Shell command to peak at queries sent to Spark Thrift Server","text":"%sh\negrep $'Running|\\x0d|limit' /data/var/log/spark2-thrift/spark-hive-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-*-1.out | tail -400\n","user":"anonymous","dateUpdated":"2017-09-21T12:46:55+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":"false"},"editorMode":"ace/mode/sh","editorHide":false,"tableHide":false,"title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1505162753735_-7871773","id":"20170911-204553_822044476","dateCreated":"2017-09-11T20:45:53+0000","dateStarted":"2017-09-21T12:46:21+0000","dateFinished":"2017-09-21T12:46:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:355","errorMessage":""},{"dateUpdated":"2017-09-05T21:23:21+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1504646601871_1886918038","id":"20170504-171816_753503470","dateCreated":"2017-09-05T21:23:21+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:356"},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1505998053409_1997597492","id":"20170921-124733_1996309865","dateCreated":"2017-09-21T12:47:33+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:357"}],"name":"xtra Connecting DV Desktop and Spark","id":"2CSBTXV7T","angularObjects":{"2CUR9TKC3:shared_process":[],"2CV6GZ6KY:shared_process":[],"2CRXRKTZS:shared_process":[],"2CUEWBE5C:shared_process":[],"2CUNV7BVE:shared_process":[],"2CRQ5UM9P:shared_process":[],"2CRHKT1V7:shared_process":[],"2C4U48MY3_spark2:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}