{"paragraphs":[{"text":"%md\n# Tutorial - Working with Data Visualization Desktop\n\nThis tutorial was built for BDCS-CE version 17.3.1.  If you are using a later version of BDCS-CE, there may be a newer version of this tutorial notebook at <https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake>.  Questions and feedback about the tutorial: <david.bayard@oracle.com>\n\n## work in progress\n\nNote: This tutorial assumes you have tables defined in the Hive metastore which you should have if you ran the earlier tutorials.\n","user":"anonymous","dateUpdated":"2017-07-28T17:16:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Tutorial - Working with Data Visualization Desktop</h1>\n<p>This tutorial was built for BDCS-CE version 17.3.1. If you are using a later version of BDCS-CE, there may be a newer version of this tutorial notebook at <a href=\"https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake\">https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake</a>. Questions and feedback about the tutorial: <a href=\"mailto:&#100;&#x61;&#118;&#105;&#100;&#x2e;&#98;&#97;&#x79;&#97;&#x72;&#x64;&#x40;&#x6f;&#114;&#x61;&#99;&#108;e&#46;&#x63;&#x6f;m\">&#100;&#x61;&#118;&#105;&#100;&#x2e;&#98;&#97;&#x79;&#97;&#x72;&#x64;&#x40;&#x6f;&#114;&#x61;&#99;&#108;e&#46;&#x63;&#x6f;m</a></p>\n<h2>work in progress</h2>\n<p>Note: This tutorial assumes you have tables defined in the Hive metastore which you should have if you ran the earlier tutorials.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501258268936_2122372679","id":"20170504-171842_974565851","dateCreated":"2017-07-28T16:11:08+0000","dateStarted":"2017-07-28T17:16:59+0000","dateFinished":"2017-07-28T17:16:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:404"},{"text":"%md\n# Download DV Desktop\n\nWe used 12.2.3.0.0 - June 2017 Update\n\nGo to <http://www.oracle.com/technetwork/middleware/oracle-data-visualization/index.html>\n![DVDownload](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0012145.jpg \"DVDownload\")\n\n\n","user":"anonymous","dateUpdated":"2017-07-28T17:23:58+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Download DV Desktop</h1>\n<p>We used 12.2.3.0.0 - June 2017 Update</p>\n<p>Go to <a href=\"http://www.oracle.com/technetwork/middleware/oracle-data-visualization/index.html\">http://www.oracle.com/technetwork/middleware/oracle-data-visualization/index.html</a><br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0012145.jpg\" alt=\"DVDownload\" title=\"DVDownload\" /></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501258268937_2121987930","id":"20170501-162459_1108177652","dateCreated":"2017-07-28T16:11:08+0000","dateStarted":"2017-07-28T17:20:04+0000","dateFinished":"2017-07-28T17:20:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:405"},{"text":"%md\n# Extract the DV Desktop zip file and run the DVDesktop.msi\n\n+ Unzip it and launch the .msi file.\n![DVInstall](https://gse00010212.storage.oraclecloud.com/v1/Storage-gse00010212/images/snap0011606.jpg \"DVInstall\")\n\n+ When it is done, click Finish\n![DVInstall2](https://gse00010212.storage.oraclecloud.com/v1/Storage-gse00010212/images/snap0011607.jpg \"DVInstall2\")\n\n+ DV Desktop will launch\n![DVInstall3](https://gse00010212.storage.oraclecloud.com/v1/Storage-gse00010212/images/snap0011608.jpg \"DVInstall3\")","user":"anonymous","dateUpdated":"2017-07-28T17:24:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Extract the DV Desktop zip file and run the DVDesktop.msi</h1>\n<ul>\n  <li>\n  <p>Unzip it and launch the .msi file.<br/><img src=\"https://gse00010212.storage.oraclecloud.com/v1/Storage-gse00010212/images/snap0011606.jpg\" alt=\"DVInstall\" title=\"DVInstall\" /></p></li>\n  <li>\n  <p>When it is done, click Finish<br/><img src=\"https://gse00010212.storage.oraclecloud.com/v1/Storage-gse00010212/images/snap0011607.jpg\" alt=\"DVInstall2\" title=\"DVInstall2\" /></p></li>\n  <li>\n  <p>DV Desktop will launch<br/><img src=\"https://gse00010212.storage.oraclecloud.com/v1/Storage-gse00010212/images/snap0011608.jpg\" alt=\"DVInstall3\" title=\"DVInstall3\" /></p></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1501258268937_2121987930","id":"20170501-162605_2114505553","dateCreated":"2017-07-28T16:11:08+0000","dateStarted":"2017-07-28T17:24:53+0000","dateFinished":"2017-07-28T17:24:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:406"},{"text":"%md\n# Installing the Hortonworks ODBC driver for Hive\n\nDV Desktop can work with ODBC drivers.  We will leverage this capability as it allows us to define some advanced parameters which will be needed to connect to our Hive instance in BDCS-CE.\n\n+ Navigate to https://hortonworks.com/downloads\n+ Under Hortonworks Data Platform Add-ons, download the Windows (64bit) version of the latest Hortonworks ODBC Driver for Apache Hive\n![HOdbcDownload](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0012146.jpg \"HODBC\")\n+ Run the downloaded MSI file to install it\n![HOdbcinstall](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/HiveODBCinstall.gif \"HODBC\")\n","user":"anonymous","dateUpdated":"2017-07-28T18:22:45+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":"true"},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Installing the Hortonworks ODBC driver for Hive</h1>\n<p>DV Desktop can work with ODBC drivers. We will leverage this capability as it allows us to define some advanced parameters which will be needed to connect to our Hive instance in BDCS-CE.</p>\n<ul>\n  <li>Navigate to <a href=\"https://hortonworks.com/downloads\">https://hortonworks.com/downloads</a></li>\n  <li>Under Hortonworks Data Platform Add-ons, download the Windows (64bit) version of the latest Hortonworks ODBC Driver for Apache Hive<br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0012146.jpg\" alt=\"HOdbcDownload\" title=\"HODBC\" /></li>\n  <li>Run the downloaded MSI file to install it<br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/HiveODBCinstall.gif\" alt=\"HOdbcinstall\" title=\"HODBC\" /></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1501262433459_796535823","id":"20170728-172033_1852387240","dateCreated":"2017-07-28T17:20:33+0000","dateStarted":"2017-07-28T18:22:45+0000","dateFinished":"2017-07-28T18:22:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:407"},{"text":"%md\n# Setup a SSH port forwarding tunnel for port 10002\n\n+ Connect to your master BDCS-CE server via SSH.  Refer to the instructions in the Tutorial \"Setting up your BDCS-CE Instance\" on how to connect via SSH.\n+ In addition, add a port forwarding tunnel from your laptop's port 10002 to the server address 0.0.0.0:10002.  Here is an example using PUTTY:\n![SSH10002](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/SSH10002.gif \"HODBC\")","user":"anonymous","dateUpdated":"2017-07-28T17:31:32+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":"true"},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Setup a SSH port forwarding tunnel for port 10002</h1>\n<ul>\n  <li>Connect to your master BDCS-CE server via SSH. Refer to the instructions in the Tutorial &ldquo;Setting up your BDCS-CE Instance&rdquo; on how to connect via SSH.</li>\n  <li>In addition, add a port forwarding tunnel from your laptop&rsquo;s port 10002 to the server address 0.0.0.0:10002. Here is an example using PUTTY:<br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/SSH10002.gif\" alt=\"SSH10002\" title=\"HODBC\" /></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1501262771501_135043550","id":"20170728-172611_1770771195","dateCreated":"2017-07-28T17:26:11+0000","dateStarted":"2017-07-28T17:31:32+0000","dateFinished":"2017-07-28T17:31:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:408"},{"text":"%md\n# Configure the Hive ODBC Driver for BDCS-CE\n\n+ Open up the Windows ODBC Data Sources (64 bit) tool.  The easiest way is to use the search icon and type in ODBC.\n![ODBClaunch](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0012147.jpg \"HODBC\")\n+ Click on the System DSN tab\n+ Click Add...\n+ Choose the Hortonworks Hive ODBC driver\n+ Enter a data source name (remember it for later)\n+ Enter 127.0.0.1 for the host\n+ Enter 10002 for the port\n+ Set the mechanism for authentication to User Name and Password\n+ Set the User Name to hive\n+ Put \"password\" into password (the exact value does not matter)\n+ Choose HTTP for the the Thrift Transport\n+ Click on HTTP Options and enter hs2service for the HTTP path\n+ Click Test to make sure it works\n+ Click OK\n![ODBClaunch](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/HiveODBCconfig.gif \"HODBC\")\n","user":"anonymous","dateUpdated":"2017-07-28T18:28:40+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":"true"},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Configure the Hive ODBC Driver for BDCS-CE</h1>\n<ul>\n  <li>Open up the Windows ODBC Data Sources (64 bit) tool. The easiest way is to use the search icon and type in ODBC.<br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0012147.jpg\" alt=\"ODBClaunch\" title=\"HODBC\" /></li>\n  <li>Click on the System DSN tab</li>\n  <li>Click Add&hellip;</li>\n  <li>Choose the Hortonworks Hive ODBC driver</li>\n  <li>Enter a data source name (remember it for later)</li>\n  <li>Enter 127.0.0.1 for the host</li>\n  <li>Enter 10002 for the port</li>\n  <li>Set the mechanism for authentication to User Name and Password</li>\n  <li>Set the User Name to hive</li>\n  <li>Put &ldquo;password&rdquo; into password (the exact value does not matter)</li>\n  <li>Choose HTTP for the the Thrift Transport</li>\n  <li>Click on HTTP Options and enter hs2service for the HTTP path</li>\n  <li>Click Test to make sure it works</li>\n  <li>Click OK<br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/HiveODBCconfig.gif\" alt=\"ODBClaunch\" title=\"HODBC\" /></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1501263100970_-1081903018","id":"20170728-173140_2001047313","dateCreated":"2017-07-28T17:31:40+0000","dateStarted":"2017-07-28T18:28:40+0000","dateFinished":"2017-07-28T18:28:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:409"},{"text":"%md\n# Define a connection in DV Desktop for the Hive ODBC connection\n\n![DVDconnection](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/DVDconnection.gif \"HODBC\")\n![DVDdatasource](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/DVDdatasource.gif \"HODBC\")\n\n","user":"anonymous","dateUpdated":"2017-07-28T17:47:48+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":"true"},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Define a connection in DV Desktop for the Hive ODBC connection</h1>\n<p><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/DVDconnection.gif\" alt=\"DVDconnection\" title=\"HODBC\" /><br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/DVDdatasource.gif\" alt=\"DVDdatasource\" title=\"HODBC\" /></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501263748968_980502684","id":"20170728-174228_1743240708","dateCreated":"2017-07-28T17:42:28+0000","dateStarted":"2017-07-28T17:47:48+0000","dateFinished":"2017-07-28T17:47:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:410"},{"text":"%md\n\n# Now trying to connect to the Spark Thrift Server\n\nThis runs on port 10001\n\n+ Download the Hortonworks Spark ODBC Windows 64-bit driver\n![ODBClaunch](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0012148.jpg \"HODBC\")\n+ Install the MSI\n\n### Setup a SSH forward for port 10001\n\n+ Local port = 10001.  Server address = 127.0.0.1:10001\n\n\n### Configure the Spark ODBC driver\n\n+ Open up the Windows ODBC Data Sources (64 bit) tool.  The easiest way is to use the search icon and type in ODBC.\n![ODBClaunch](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0012147.jpg \"HODBC\")\n+ Click on the System DSN tab\n+ Click Add...\n+ Choose the Hortonworks Spark ODBC driver\n+ Enter a data source name (remember it for later)\n+ Enter 127.0.0.1 for the host\n+ Enter 10001 for the port\n+ Set the mechanism for authentication to User Name and Password\n+ Set the User Name to hive\n+ Put \"password\" into password (the exact value does not matter)\n+ Choose HTTP for the the Thrift Transport\n+ Click on HTTP Options and enter cliservice for the HTTP path\n+ Click on the Advanced Options button\n+ Make sure \"Get Tables with Query\" is checked.  Click OK to close the Advanced Options dialog\n+ Click Test to make sure it works\n+ Click OK","user":"anonymous","dateUpdated":"2017-07-28T18:30:11+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":"true"},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501266253745_-1589269157","id":"20170728-182413_2140745030","dateCreated":"2017-07-28T18:24:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1382","dateFinished":"2017-07-28T18:29:53+0000","dateStarted":"2017-07-28T18:29:53+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Now trying to connect to the Spark Thrift Server</h1>\n<p>This runs on port 10001</p>\n<ul>\n  <li>Download the Hortonworks Spark ODBC Windows 64-bit driver<br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0012148.jpg\" alt=\"ODBClaunch\" title=\"HODBC\" /></li>\n  <li>Install the MSI</li>\n</ul>\n<h3>Setup a SSH forward for port 10001</h3>\n<ul>\n  <li>Local port = 10001. Server address = 127.0.0.1:10001</li>\n</ul>\n<h3>Configure the Spark ODBC driver</h3>\n<ul>\n  <li>Open up the Windows ODBC Data Sources (64 bit) tool. The easiest way is to use the search icon and type in ODBC.<br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0012147.jpg\" alt=\"ODBClaunch\" title=\"HODBC\" /></li>\n  <li>Click on the System DSN tab</li>\n  <li>Click Add&hellip;</li>\n  <li>Choose the Hortonworks Spark ODBC driver</li>\n  <li>Enter a data source name (remember it for later)</li>\n  <li>Enter 127.0.0.1 for the host</li>\n  <li>Enter 10001 for the port</li>\n  <li>Set the mechanism for authentication to User Name and Password</li>\n  <li>Set the User Name to hive</li>\n  <li>Put &ldquo;password&rdquo; into password (the exact value does not matter)</li>\n  <li>Choose HTTP for the the Thrift Transport</li>\n  <li>Click on HTTP Options and enter cliservice for the HTTP path</li>\n  <li>Click on the Advanced Options button</li>\n  <li>Make sure &ldquo;Get Tables with Query&rdquo; is checked. Click OK to close the Advanced Options dialog</li>\n  <li>Click Test to make sure it works</li>\n  <li>Click OK</li>\n</ul>\n</div>"}]}},{"text":"%md\n# Issues with Spark SQL\n\nWith a large # of columns, I get these errors from DVD (errors found at /data/var/log/spark-thrift/spark-hive-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-bdcsce-lab-july2017-bdcsce-1.out\n)\n\norg.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 0, required: 168. To avoid this, increase spark.kryoserializer.buffer.max value.\n        at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:299)\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:240)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:246)\n        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:154)\n        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:151)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)\n        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:164)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:748)\n\n","user":"anonymous","dateUpdated":"2017-07-28T18:32:01+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":"true"},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501266614194_354034596","id":"20170728-183014_557439817","dateCreated":"2017-07-28T18:30:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1607","dateFinished":"2017-07-28T18:32:02+0000","dateStarted":"2017-07-28T18:32:01+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Issues with Spark SQL</h1>\n<p>With a large # of columns, I get these errors from DVD (errors found at /data/var/log/spark-thrift/spark-hive-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-bdcsce-lab-july2017-bdcsce-1.out<br/>)</p>\n<p>org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 0, required: 168. To avoid this, increase spark.kryoserializer.buffer.max value.<br/> at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:299)<br/> at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:240)<br/> at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)<br/> at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)<br/> at java.lang.Thread.run(Thread.java:748)</p>\n<p>Driver stacktrace:<br/> at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:246)<br/> at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:154)<br/> at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:151)<br/> at java.security.AccessController.doPrivileged(Native Method)<br/> at javax.security.auth.Subject.doAs(Subject.java:422)<br/> at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)<br/> at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:164)<br/> at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)<br/> at java.util.concurrent.FutureTask.run(FutureTask.java:266)<br/> at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)<br/> at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)<br/> at java.lang.Thread.run(Thread.java:748)</p>\n</div>"}]}},{"dateUpdated":"2017-07-28T16:11:08+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501258268939_2122757427","id":"20170504-171816_753503470","dateCreated":"2017-07-28T16:11:08+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:417"}],"name":"Tutorial X Working with DV Desktop and Hive","id":"2CRMQZCVR","angularObjects":{"2CQVFQFWU:shared_process":[],"2CRFREMNU:shared_process":[],"2CPRZVQKR:shared_process":[],"2CQWU46XZ:shared_process":[],"2CPKCF4R9:shared_process":[],"2CR7Y2CFK:shared_process":[],"2CQQM64RT:shared_process":[],"2CQX3UW3S:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}