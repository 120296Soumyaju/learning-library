{"paragraphs":[{"text":"%md\n# DataSci Tutorial 2: Using R with Zeppelin\n\nThis tutorial was built for BDCS-CE version 17.3.1-20 as part of the Data Science Acceleration User Journey: <a href=\"https://oracle.github.io/learning-library/workshops/journey3-data-science/\" target=\"_blank\">here</a>.  Questions and feedback about the tutorial: <david.bayard@oracle.com>\n\n    Be sure you previously ran the Tutorial \"Setup R, SparkR, RStudio Server\".\n\nThis tutorial provides some examples of using R and SparkR in Zeppelin notebooks.  It will show:\n\n+ How to query a hive table from R\n+ How to read data directly from the Object Store\n+ How to convert a R data.frame into a Spark Temporary Table and query it with SparkSQL\n+ Machine Learning with R and Spark\n+ Save results back to the Object Store\n\n","user":"anonymous","dateUpdated":"2017-08-11T19:20:08+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":"true"},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>DataSci Tutorial 2: Using R with Zeppelin</h1>\n<p>This tutorial was built for BDCS-CE version 17.3.1-20 as part of the Data Science Acceleration User Journey: <a href=\"https://oracle.github.io/learning-library/workshops/journey3-data-science/\" target=\"_blank\">here</a>. Questions and feedback about the tutorial: <a href=\"mailto:&#100;a&#118;&#x69;&#x64;&#x2e;&#x62;&#97;&#x79;&#x61;r&#100;&#x40;&#111;r&#97;&#99;&#108;e&#46;c&#x6f;&#109;\">&#100;a&#118;&#x69;&#x64;&#x2e;&#x62;&#97;&#x79;&#x61;r&#100;&#x40;&#111;r&#97;&#99;&#108;e&#46;c&#x6f;&#109;</a></p>\n<pre><code>Be sure you previously ran the Tutorial &quot;Setup R, SparkR, RStudio Server&quot;.\n</code></pre>\n<p>This tutorial provides some examples of using R and SparkR in Zeppelin notebooks. It will show:</p>\n<ul>\n  <li>How to query a hive table from R</li>\n  <li>How to read data directly from the Object Store</li>\n  <li>How to convert a R data.frame into a Spark Temporary Table and query it with SparkSQL</li>\n  <li>Machine Learning with R and Spark</li>\n  <li>Save results back to the Object Store</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1502476727462_920826787","id":"20170811-183847_958683840","dateCreated":"2017-08-11T18:38:47+0000","dateStarted":"2017-08-11T19:20:08+0000","dateFinished":"2017-08-11T19:20:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:418"},{"text":"%md\n# About R and Zeppelin\n\nZeppelin includes an interpreter that is integrated with R and SparkR.  You can find some details about Zeppelin and R <a href=\"https://zeppelin.apache.org/docs/0.7.0/interpreter/r.html\" target=\"_blank\">here</a>.  You can find some details about SparkR <a href=\"https://spark.apache.org/docs/2.1.0/sparkr.html\" target=\"_blank\">here</a>.\n\nThese examples focuses on using R to work with Spark features like DataFrames.  As the SparkR documentation writes, \"A SparkDataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R, but with richer optimizations under the hood. SparkDataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing local R data frames\".  This tutorial will demonstrate some of these capabilities.\n\n\n\n","user":"anonymous","dateUpdated":"2017-08-11T19:22:00+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>About R and Zeppelin</h1>\n<p>Zeppelin includes an interpreter that is integrated with R and SparkR. You can find some details about Zeppelin and R <a href=\"https://zeppelin.apache.org/docs/0.7.0/interpreter/r.html\" target=\"_blank\">here</a>. You can find some details about SparkR <a href=\"https://spark.apache.org/docs/2.1.0/sparkr.html\" target=\"_blank\">here</a>.</p>\n<p>These examples focuses on using R to work with Spark features like DataFrames. As the SparkR documentation writes, &ldquo;A SparkDataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R, but with richer optimizations under the hood. SparkDataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing local R data frames&rdquo;. This tutorial will demonstrate some of these capabilities.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1502462722748_-1950272520","id":"20170807-213713_1708624789","dateCreated":"2017-08-11T14:45:22+0000","dateStarted":"2017-08-11T19:22:00+0000","dateFinished":"2017-08-11T19:22:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:419"},{"text":"%md\n# Example of R querying Hive\n\nThis example shows how to use R to query the bike_trips hive table via SparkR features.\n","user":"anonymous","dateUpdated":"2017-08-11T18:45:44+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Example of R querying Hive</h1>\n<p>This example shows how to use R to query the bike_trips hive table via SparkR features.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1502462722749_-1950657269","id":"20170810-001234_1882189757","dateCreated":"2017-08-11T14:45:22+0000","dateStarted":"2017-08-11T18:45:44+0000","dateFinished":"2017-08-11T18:45:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:420"},{"title":"SparkR to query a Hive table","text":"%r\nresults <- sql(\"SELECT * from bike_trips\")\nhead(results)\n","user":"anonymous","dateUpdated":"2017-08-11T18:51:05+0000","config":{"tableHide":false,"editorSetting":{"language":"r","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/r","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1502462722750_-1949503022","id":"20170810-001354_1251372462","dateCreated":"2017-08-11T14:45:22+0000","dateStarted":"2017-08-11T18:50:56+0000","dateFinished":"2017-08-11T18:51:01+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:421"},{"text":"%r\n# let's see what kind of class our results are...\nresults\n# It is a SparkDataFrame","user":"anonymous","dateUpdated":"2017-08-11T18:57:20+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1502477469603_1877556619","id":"20170811-185109_995138400","dateCreated":"2017-08-11T18:51:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1688","dateFinished":"2017-08-11T18:57:20+0000","dateStarted":"2017-08-11T18:57:20+0000","errorMessage":""},{"text":"%md\n# Example of reading a CSV from Object Store\n\nThis example shows SparkR features to read a CSV from Object Store via Spark's DataSources mechanisms\n\n\n","dateUpdated":"2017-08-11T14:45:22+0000","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Example of reading a CSV from Object Store</h1>\n<p>This example shows SparkR features to read a CSV from Object Store via Spark&rsquo;s DataSources mechanisms</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1502462722750_-1949503022","id":"20170810-001731_1366310690","dateCreated":"2017-08-11T14:45:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:422"},{"text":"%r\nbiketrips <- read.df(\"swift://journeyC.default/citibike/raw/201612-citibike-tripdata.csv\", \"csv\", header = \"true\", inferSchema = \"true\", na.strings = \"NA\")\nhead(biketrips)\n","user":"anonymous","dateUpdated":"2017-08-11T14:47:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/r","results":{},"enabled":true,"editorSetting":{"language":"r","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1502462722750_-1949503022","id":"20170810-001850_158683843","dateCreated":"2017-08-11T14:45:22+0000","dateStarted":"2017-08-11T14:47:03+0000","dateFinished":"2017-08-11T14:47:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:423","errorMessage":""},{"text":"%md\n# Example of making a R dataframe into a SparkSQL table\n\nHere is an example of converting a R data.frame into a Spark DataFrame and registering it as a Spark SQL table.  You can more examples like this <a href=\"https://rpubs.com/wendyu/sparkr\" target=\"_blank\">here</a>.\n","dateUpdated":"2017-08-11T18:56:51+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Example of making a R dataframe into a SparkSQL table</h1>\n<p>Here is an example of converting a R data.frame into a Spark DataFrame and registering it as a Spark SQL table. You can more examples like this <a href=\"https://rpubs.com/wendyu/sparkr\" target=\"_blank\">here</a>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1502462722751_-1949887771","id":"20170810-002107_611682017","dateCreated":"2017-08-11T14:45:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:424","user":"anonymous","dateFinished":"2017-08-11T18:56:53+0000","dateStarted":"2017-08-11T18:56:51+0000"},{"title":"Load an R data frame","text":"%r\ndata(iris)\niris","dateUpdated":"2017-08-11T18:55:43+0000","config":{"editorSetting":{"language":"r"},"colWidth":12,"editorMode":"ace/mode/r","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1502462722751_-1949887771","id":"20170808-174600_955896680","dateCreated":"2017-08-11T14:45:22+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:425","user":"anonymous","dateFinished":"2017-08-11T18:55:43+0000","dateStarted":"2017-08-11T18:55:43+0000"},{"title":"SparkR code to register an R dataframe as a SparkSQL table","text":"%r\nirisDF <- as.DataFrame(iris)\nregisterTempTable(irisDF,\"iris\")\nirisDF\n","dateUpdated":"2017-08-11T18:57:50+0000","config":{"editorSetting":{"language":"r"},"colWidth":12,"editorMode":"ace/mode/r","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1502462722752_-1964123480","id":"20170808-185507_1332635419","dateCreated":"2017-08-11T14:45:22+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:426","user":"anonymous","dateFinished":"2017-08-11T18:57:51+0000","dateStarted":"2017-08-11T18:57:50+0000"},{"title":"SparkSQL querying R data","text":"%sql\nselect * from iris\n\n","dateUpdated":"2017-08-11T18:57:58+0000","config":{"editorSetting":{"language":"sql"},"colWidth":12,"editorMode":"ace/mode/sql","title":true,"results":{"0":{"graph":{"mode":"scatterChart","height":300,"optionOpen":false,"setting":{"scatterChart":{"xAxis":{"name":"Sepal_Length","index":0,"aggr":"sum"},"yAxis":{"name":"Sepal_Width","index":1,"aggr":"sum"},"group":{"name":"Species","index":4,"aggr":"sum"}}}},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1502462722752_-1964123480","id":"20170810-002439_1141063356","dateCreated":"2017-08-11T14:45:22+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:427","user":"anonymous","dateFinished":"2017-08-11T18:58:02+0000","dateStarted":"2017-08-11T18:57:59+0000"},{"text":"%md\n# Machine Learning with R and Spark\n\nThis example shows running a Spark machine learning algorithm - Generalized Linear Model (glm).\n\nWe will use our citibike data and model tripduration based on age and gender.\n","dateUpdated":"2017-08-11T19:00:14+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Machine Learning with R and Spark</h1>\n<p>This example shows running a Spark machine learning algorithm - Generalized Linear Model (glm).</p>\n<p>We will use our citibike data and model tripduration based on age and gender.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1502462722753_-1964508229","id":"20170810-004712_451598661","dateCreated":"2017-08-11T14:45:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:428","user":"anonymous","dateFinished":"2017-08-11T19:00:14+0000","dateStarted":"2017-08-11T19:00:14+0000"},{"title":"SparkR code to build generalized linear model (GLM) of tripduration based on gender and age","text":"%r\nageGender  <- sql(\"SELECT tripduration, (2016-birthyear) age, gender from bike_trips\")\ntraining <- dropna(ageGender)\n\nmodel <- glm(tripduration ~ age + gender,\n    family = \"gaussian\", data = training)\nsummary(model)","dateUpdated":"2017-08-11T19:02:06+0000","config":{"editorSetting":{"language":"r","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/r","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1502462722753_-1964508229","id":"20170810-004706_1398119785","dateCreated":"2017-08-11T14:45:22+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:429","user":"anonymous","dateFinished":"2017-08-11T19:02:47+0000","dateStarted":"2017-08-11T19:01:58+0000"},{"title":"Check our predictions (not so good)","text":"%r\nfitted <- predict(model, training)\nregisterTempTable(fitted,\"fitted\")\ncompare <- sql(\"select prediction, tripduration, age, gender from fitted\")\nhead(compare)\n","dateUpdated":"2017-08-11T19:03:29+0000","config":{"editorSetting":{"language":"r"},"colWidth":12,"editorMode":"ace/mode/r","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1502462722753_-1964508229","id":"20170810-010230_308509598","dateCreated":"2017-08-11T14:45:22+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:430","user":"anonymous","dateFinished":"2017-08-11T19:03:30+0000","dateStarted":"2017-08-11T19:03:29+0000"},{"title":"SparkSQL to view predictions","text":"%sql\nselect prediction, tripduration, gender, age from fitted\nlimit 100\n","dateUpdated":"2017-08-11T19:03:58+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sql","title":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":true,"setting":{"lineChart":{},"scatterChart":{"xAxis":{"name":"prediction","index":0,"aggr":"sum"},"yAxis":{"name":"tripduration","index":1,"aggr":"sum"},"group":{"name":"gender","index":2,"aggr":"sum"}},"stackedAreaChart":{}},"keys":[],"groups":[],"values":[{"name":"tripduration","index":1,"aggr":"sum"},{"name":"prediction","index":0,"aggr":"sum"}],"commonSetting":{}},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1502462722754_-1963353983","id":"20170808-202112_1798282605","dateCreated":"2017-08-11T14:45:22+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:431","user":"anonymous","dateFinished":"2017-08-11T19:03:58+0000","dateStarted":"2017-08-11T19:03:58+0000"},{"text":"%md\n# Example of writing a DataFrame back to Object Store\n\nThe follow shows an example of writing a DataFrame back to the Object Store.  We use the write.df method from SparkR.  It supports multiple source types (csv, json, parquet, etc).","user":"anonymous","dateUpdated":"2017-08-11T19:19:16+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/text","editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1502478901962_1980502403","id":"20170811-191501_1517148443","dateCreated":"2017-08-11T19:15:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2537","dateFinished":"2017-08-11T19:17:31+0000","dateStarted":"2017-08-11T19:17:31+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Example of writing a DataFrame back to Object Store</h1>\n<p>The follow shows an example of writing a DataFrame back to the Object Store. We use the write.df method from SparkR. It supports multiple source types (csv, json, parquet, etc).</p>\n</div>"}]}},{"text":"%r\n# Since we know the resulting file is small, we will do a repartition command to force Spark to write the output as a single file.  This is an optional step.\nfitted_singlepartition <- repartition(fitted,1)\nwrite.df(fitted_singlepartition, \"swift://journeyC.default/citibike/results/201612-fitted-projections\", source=\"csv\", mode=\"overwrite\")\n","user":"anonymous","dateUpdated":"2017-08-11T19:29:17+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1502478432460_1553742541","id":"20170811-190712_164268436","dateCreated":"2017-08-11T19:07:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2421","dateFinished":"2017-08-11T19:29:47+0000","dateStarted":"2017-08-11T19:29:17+0000","title":"R code to write our predictions back to the Object Store","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"\n\n\n\n"}]}},{"text":"%sh\n# this command will show you the contents of the Object Store that were just written\nhadoop fs -ls swift://journeyC.default/citibike/results/201612-fitted-projections\n","dateUpdated":"2017-08-11T19:24:30+0000","config":{"colWidth":12,"editorMode":"ace/mode/text","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false},"title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1502462722754_-1963353983","id":"20170807-214237_845297194","dateCreated":"2017-08-11T14:45:22+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:432","user":"anonymous","dateFinished":"2017-08-11T19:24:33+0000","dateStarted":"2017-08-11T19:24:30+0000","title":"Explore the contents of the Object Store"},{"text":"%md\n### Change Log\nAugust 11, 2017 - First version\n","user":"anonymous","dateUpdated":"2017-08-11T19:19:58+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":"true"},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1502478983625_-288823231","id":"20170811-191623_826620851","dateCreated":"2017-08-11T19:16:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2615","dateFinished":"2017-08-11T19:19:58+0000","dateStarted":"2017-08-11T19:19:58+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Change Log</h3>\n<p>August 11, 2017 - First version</p>\n</div>"}]}},{"text":"%md\n","user":"anonymous","dateUpdated":"2017-08-11T19:19:58+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":"true"},"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1502479198013_-1256175555","id":"20170811-191958_1291497317","dateCreated":"2017-08-11T19:19:58+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2884"}],"name":"DataSci Tutorial 2 Using R with Zeppelin","id":"2CSUAV3HC","angularObjects":{"2CQV2CBDJ:shared_process":[],"2CSEK8ZNM:shared_process":[],"2CR6M4PTD:shared_process":[],"2CPFF54HX:shared_process":[],"2CSTM8NSJ:shared_process":[],"2CQ7GW4U4:shared_process":[],"2CT3QR1E8:shared_process":[],"2C4U48MY3_spark2:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}